{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eQPnKyYsShom"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets trl huggingface_hub -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxMR4p7VSfAl"
      },
      "source": [
        "# Preference Alignment with Direct Preference Optimization (DPO)\n",
        "\n",
        "This notebook will guide you through the process of fine-tuning a language model using Direct Preference Optimization (DPO). We will use the SmolLM2-135M-Instruct model which has already been through a SFT training, so it it compatible with DPO. You can also use the model you trained in [1_instruction_tuning](../../1_instruction_tuning/notebooks/sft_finetuning_example.ipynb).\n",
        "\n",
        "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
        "     <h2 style='margin: 0;color:blue'>Exercise: Aligning SmolLM2 with DPOTrainer</h2>\n",
        "     <p>Take a dataset from the Hugging Face hub and align a model on it. </p>\n",
        "     <p><b>Difficulty Levels</b></p>\n",
        "     <p>üê¢ Use the `trl-lib/ultrafeedback_binarized` dataset</p>\n",
        "     <p>üêï Try out the `argilla/ultrafeedback-binarized-preferences` dataset</p>\n",
        "     <p>ü¶Å Select a dataset that relates to a real-world use case you‚Äôre interested in, or use the model you trained in\n",
        "        <a href=\"../../1_instruction_tuning/notebooks/sft_finetuning_example.ipynb\">1_instruction_tuning</a></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oQMN0qUsSfAn"
      },
      "outputs": [],
      "source": [
        "# Install the requirements in Google Colab\n",
        "\n",
        "# Authenticate to Hugging Face\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\"HF_Token\")\n",
        "\n",
        "# for convenience you can create an environment variable containing your hub token as HF_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPUD6P4qSfAo"
      },
      "source": [
        "## Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_zIBL8IssExG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from trl import DPOTrainer, DPOConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8CvUgROUDw-"
      },
      "source": [
        "## Format dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCD77GZ60DOT",
        "outputId": "5138d392-ec87-4800-c9e5-51848ba8d1bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "\n",
        "# TODO: ü¶Åüêï change the dataset to one of your choosing\n",
        "dataset = load_dataset(path=\"trl-lib/ultrafeedback_binarized\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5o_aIvm0SfAp"
      },
      "outputs": [],
      "source": [
        "# TODO: üêï If your dataset is not represented as conversation lists, you can use the `process_dataset` function to convert it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTzsZqHSSfAp"
      },
      "source": [
        "## Select the model\n",
        "\n",
        "We will use the SmolLM2-135M-Instruct model which has already been through a SFT training, so it it compatible with DPO. You can also use the model you trained in [1_instruction_tuning](../../1_instruction_tuning/notebooks/sft_finetuning_example.ipynb).\n",
        "\n",
        "\n",
        "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; width:80%; color:black'>\n",
        "     <p>ü¶Å change the model to the path or repo id of the model you trained in <a href=\"../../1_instruction_tuning/notebooks/sft_finetuning_example.ipynb\">1_instruction_tuning</a></p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "98KvJ0BUSfAp"
      },
      "outputs": [],
      "source": [
        "# TODO: ü¶Å change the model to the path or repo id of the model you trained in [1_instruction_tuning](../../1_instruction_tuning/notebooks/sft_finetuning_example.ipynb)\n",
        "\n",
        "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Model to fine-tune\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name,\n",
        "    torch_dtype=torch.float32,\n",
        ").to(device)\n",
        "model.config.use_cache = False\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Set our name for the finetune to be saved &/ uploaded to\n",
        "finetune_name = \"SmolLM2-FT-DPO\"\n",
        "finetune_tags = [\"smol-course\", \"module_1\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeT5eUK_UJgK"
      },
      "source": [
        "## Train model with DPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rKPILNOLR-aK"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = DPOConfig(\n",
        "    # Training batch size per GPU\n",
        "    per_device_train_batch_size=4,\n",
        "    # Number of updates steps to accumulate before performing a backward/update pass\n",
        "    # Effective batch size = per_device_train_batch_size * gradient_accumulation_steps\n",
        "    gradient_accumulation_steps=4,\n",
        "    # Saves memory by not storing activations during forward pass\n",
        "    # Instead recomputes them during backward pass\n",
        "    gradient_checkpointing=True,\n",
        "    # Base learning rate for training\n",
        "    learning_rate=5e-5,\n",
        "    # Learning rate schedule - 'cosine' gradually decreases LR following cosine curve\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    # Total number of training steps\n",
        "    max_steps=15,\n",
        "    # Disables model checkpointing during training\n",
        "    save_strategy=\"no\",\n",
        "    # How often to log training metrics\n",
        "    logging_steps=1,\n",
        "    # Directory to save model outputs\n",
        "    output_dir=\"smol_dpo_output\",\n",
        "    # Number of steps for learning rate warmup\n",
        "    warmup_steps=100,\n",
        "    # Use bfloat16 precision for faster training\n",
        "    bf16=True,\n",
        "    # Disable wandb/tensorboard logging\n",
        "    report_to=\"none\",\n",
        "    # Keep all columns in dataset even if not used\n",
        "    remove_unused_columns=False,\n",
        "    # Enable MPS (Metal Performance Shaders) for Mac devices\n",
        "    use_mps_device=device == \"mps\",\n",
        "    # Model ID for HuggingFace Hub uploads\n",
        "    hub_model_id=finetune_name,\n",
        "    # DPO-specific temperature parameter that controls the strength of the preference model\n",
        "    # Lower values (like 0.1) make the model more conservative in following preferences\n",
        "    beta=0.1,\n",
        "    # Maximum length of the input prompt in tokens\n",
        "    max_prompt_length=1024,\n",
        "    # Maximum combined length of prompt + response in tokens\n",
        "    max_length=1024,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9hs_mwSuSfAq"
      },
      "outputs": [],
      "source": [
        "trainer = DPOTrainer(\n",
        "    # The model to be trained\n",
        "    model=model,\n",
        "    # Training configuration from above\n",
        "    args=training_args,\n",
        "    # Dataset containing preferred/rejected response pairs\n",
        "    train_dataset=dataset,\n",
        "    # Tokenizer for processing inputs\n",
        "    processing_class=tokenizer,\n",
        "    # DPO-specific temperature parameter that controls the strength of the preference model\n",
        "    # Lower values (like 0.1) make the model more conservative in following preferences\n",
        "    # beta=0.1,\n",
        "    # Maximum length of the input prompt in tokens\n",
        "    # max_prompt_length=1024,\n",
        "    # Maximum combined length of prompt + response in tokens\n",
        "    # max_length=1536,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "_FhlwrsnSfAq",
        "outputId": "cc76baa0-5d89-4485-f380-863a99dc122f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 05:10, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.693100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.693100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.688400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.703700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.691800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.707000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.691200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.712700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.696900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.691400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.691200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.694200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.687000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.683200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.688800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model(f\"./{finetune_name}\")\n",
        "\n",
        "# Save to the huggingface hub if login (HF_TOKEN is set)\n",
        "if os.getenv(\"HF_TOKEN\"):\n",
        "    trainer.push_to_hub(tags=finetune_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RuXRum2SfAq"
      },
      "source": [
        "## üíê You're done!\n",
        "\n",
        "This notebook provided a step-by-step guide to fine-tuning the `HuggingFaceTB/SmolLM2-135M` model using the `DPOTrainer`. By following these steps, you can adapt the model to perform specific tasks more effectively. If you want to carry on working on this course, here are steps you could try out:\n",
        "\n",
        "- Try this notebook on a harder difficulty\n",
        "- Review a colleagues PR\n",
        "- Improve the course material via an Issue or PR."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
